{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Trump Press Briefing Analysis\n",
       "\n",
       "This notebook analyzes the frequency of specific phrases in press briefings to understand key topics and messaging patterns."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Setup"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import os\n",
       "import re\n",
       "from datetime import datetime\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "from scipy import stats\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "\n",
       "# Set style for plots\n",
       "plt.style.use('default')\n",
       "sns.set_theme(style='whitegrid')\n",
       "plt.rcParams['figure.figsize'] = [12, 6]\n",
       "\n",
       "# Set pandas display options\n",
       "pd.set_option('display.max_rows', None)\n",
       "pd.set_option('display.max_colwidth', None)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "INAUGURATION_DATE = datetime(2025, 1, 20)\n",
       "CONTEXT_WINDOW = 200  # words to display for context"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Define Search Phrases\n",
       "\n",
       "We'll analyze the following key phrases in press briefings:\n",
       "- Elon / Musk\n",
       "- DOGE / Department of Government Efficiency / Doge\n",
       "- Russia\n",
       "- Ukraine\n",
       "- Israel\n",
       "- Kash / Patel\n",
       "- America First\n",
       "- Inflation"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "SEARCH_PHRASES = {\n",
       "    'Elon': r'\\b(elon|elon\\'s|musk|musk\\'s)\\b',\n",
       "    'DOGE': r'\\b(doge|doge\\'s|department\\s+of\\s+government\\s+efficiency)\\b',\n",
       "    'Russia': r'\\b(russia|russia\\'s|russian|russians)\\b',\n",
       "    'Ukraine': r'\\b(ukraine|ukraine\\'s|ukrainian|ukrainians)\\b',\n",
       "    'Israel': r'\\b(israel|israel\\'s|israeli|israelis)\\b',\n",
       "    'Kash': r'\\b(kash|kash\\'s|patel|patel\\'s)\\b',\n",
       "    'America First': r'\\b(america\\s+first)\\b',\n",
       "    'Inflation': r'\\b(inflation|inflationary|inflated)\\b'\n",
       "}"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def count_phrases(text, phrases=SEARCH_PHRASES):\n",
       "    \"\"\"Count occurrences of phrases in text\"\"\"\n",
       "    counts = {}\n",
       "    for name, pattern in phrases.items():\n",
       "        counts[name] = len(re.findall(pattern, text.lower()))\n",
       "    return counts\n",
       "\n",
       "def get_date_from_filename(filename):\n",
       "    \"\"\"Extract date from filename format YYYY-MM-DD_...\"\"\"\n",
       "    date_str = filename.split('_')[0]\n",
       "    return datetime.strptime(date_str, '%Y-%m-%d')\n",
       "\n",
       "def read_transcript(filepath):\n",
       "    \"\"\"Read and return transcript text\"\"\"\n",
       "    with open(filepath, 'r', encoding='utf-8') as f:\n",
       "        return f.read()\n",
       "\n",
       "def find_phrase_context(text, pattern, window=CONTEXT_WINDOW):\n",
       "    \"\"\"Find phrase in text with surrounding context\"\"\"\n",
       "    matches = []\n",
       "    for match in re.finditer(pattern, text.lower()):\n",
       "        start = max(0, match.start() - window)\n",
       "        end = min(len(text), match.end() + window)\n",
       "        context = text[start:end]\n",
       "        # Add ellipsis if we're not at the start/end of the text\n",
       "        if start > 0:\n",
       "            context = '...' + context\n",
       "        if end < len(text):\n",
       "            context = context + '...'\n",
       "        matches.append(context)\n",
       "    return matches\n",
       "\n",
       "def process_directory(directory):\n",
       "    \"\"\"Process transcripts from press briefing directory only\"\"\"\n",
       "    results = []\n",
       "    \n",
       "    for root, _, files in os.walk(directory):\n",
       "        category = os.path.basename(root)\n",
       "        if category != 'press briefing':  # Only process press briefings\n",
       "            continue\n",
       "            \n",
       "        for file in files:\n",
       "            if file.endswith('.txt'):\n",
       "                filepath = os.path.join(root, file)\n",
       "                try:\n",
       "                    date = get_date_from_filename(file)\n",
       "                    text = read_transcript(filepath)\n",
       "                    counts = count_phrases(text)\n",
       "                    \n",
       "                    results.append({\n",
       "                        'date': date,\n",
       "                        'file': file,\n",
       "                        'text_length': len(text.split()),\n",
       "                        'text': text,  # Store full text for context analysis\n",
       "                        **counts\n",
       "                    })\n",
       "                except Exception as e:\n",
       "                    print(f\"Error processing {filepath}: {str(e)}\")\n",
       "    \n",
       "    return pd.DataFrame(results)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Process press briefing transcripts\n",
       "df = process_directory('../data/processed-transcripts')\n",
       "df = df.sort_values('date')\n",
       "\n",
       "# Split data by date\n",
       "df_pre = df[df['date'] < INAUGURATION_DATE]\n",
       "df_post = df[df['date'] >= INAUGURATION_DATE]\n",
       "\n",
       "# Print dataset statistics\n",
       "print(\"Dataset Statistics:\")\n",
       "print(f\"Pre-inauguration briefings: {len(df_pre)}\")\n",
       "print(f\"Post-inauguration briefings: {len(df_post)}\")\n",
       "\n",
       "print(\"\\nWord Count Statistics:\")\n",
       "print(f\"Pre-inauguration average length: {df_pre['text_length'].mean():.0f} words\")\n",
       "print(f\"Post-inauguration average length: {df_post['text_length'].mean():.0f} words\")\n",
       "\n",
       "print(\"\\nDate Ranges:\")\n",
       "print(f\"Pre-inauguration: {df_pre['date'].min().strftime('%Y-%m-%d')} to {df_pre['date'].max().strftime('%Y-%m-%d')}\")\n",
       "if not df_post.empty:\n",
       "    print(f\"Post-inauguration: {df_post['date'].min().strftime('%Y-%m-%d')} to {df_post['date'].max().strftime('%Y-%m-%d')}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Phrase Frequency Analysis"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def plot_phrase_frequency_over_time(df, phrase, window=30):\n",
       "    \"\"\"Plot the frequency of a phrase over time\"\"\"\n",
       "    plt.figure(figsize=(15, 6))\n",
       "    \n",
       "    # Calculate frequency per 1000 words\n",
       "    df = df.copy()\n",
       "    df[f'{phrase}_freq'] = (df[phrase] / df['text_length']) * 1000\n",
       "    series = df.set_index('date')[f'{phrase}_freq']\n",
       "    rolling = series.rolling(window=f'{window}D', min_periods=1).mean()\n",
       "    \n",
       "    # Plot individual points and rolling average\n",
       "    plt.scatter(series.index, series.values, alpha=0.3, color='blue', label='Individual Briefings')\n",
       "    plt.plot(rolling.index, rolling.values, 'r-', linewidth=2, label=f'{window}-day Average')\n",
       "    \n",
       "    plt.title(f'Frequency of \"{phrase}\" in Press Briefings')\n",
       "    plt.xlabel('Date')\n",
       "    plt.ylabel('Occurrences per 1000 words')\n",
       "    plt.legend()\n",
       "    plt.grid(True, alpha=0.3)\n",
       "    \n",
       "    # Add vertical line for inauguration\n",
       "    plt.axvline(x=INAUGURATION_DATE, color='k', linestyle='--', alpha=0.5, label='Inauguration')\n",
       "    \n",
       "    plt.tight_layout()\n",
       "    plt.show()\n",
       "\n",
       "# Plot time series for all phrases\n",
       "for phrase in SEARCH_PHRASES.keys():\n",
       "    plot_phrase_frequency_over_time(df, phrase)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Recent Usage in Context"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def get_recent_contexts(df, phrase, n=5):\n",
       "    \"\"\"Get the n most recent contexts for a phrase\"\"\"\n",
       "    all_matches = []\n",
       "    \n",
       "    for _, row in df.sort_values('date', ascending=False).iterrows():\n",
       "        matches = find_phrase_context(row['text'], SEARCH_PHRASES[phrase])\n",
       "        for match in matches:\n",
       "            all_matches.append({\n",
       "                'date': row['date'],\n",
       "                'context': match\n",
       "            })\n",
       "        if len(all_matches) >= n:\n",
       "            break\n",
       "    \n",
       "    return pd.DataFrame(all_matches[:n])\n",
       "\n",
       "# Get recent contexts for each phrase\n",
       "for phrase in SEARCH_PHRASES.keys():\n",
       "    contexts = get_recent_contexts(df, phrase)\n",
       "    if not contexts.empty:\n",
       "        print(f\"\\n=== Recent usage of '{phrase}' ===\\n\")\n",
       "        for _, row in contexts.iterrows():\n",
       "            print(f\"Date: {row['date'].strftime('%Y-%m-%d')}\")\n",
       "            print(f\"Context: {row['context']}\\n\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Phrase Co-occurrence Analysis"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create co-occurrence matrix\n",
       "phrase_cols = list(SEARCH_PHRASES.keys())\n",
       "cooccurrence = np.zeros((len(phrase_cols), len(phrase_cols)))\n",
       "\n",
       "for i, phrase1 in enumerate(phrase_cols):\n",
       "    for j, phrase2 in enumerate(phrase_cols):\n",
       "        if i != j:\n",
       "            # Count briefings where both phrases appear\n",
       "            cooccurrence[i,j] = ((df[phrase1] > 0) & (df[phrase2] > 0)).sum()\n",
       "\n",
       "# Create and display heatmap\n",
       "plt.figure(figsize=(12, 10))\n",
       "sns.heatmap(cooccurrence, \n",
       "            xticklabels=phrase_cols,\n",
       "            yticklabels=phrase_cols,\n",
       "            annot=True,\n",
       "            fmt='.0f',\n",
       "            cmap='YlOrRd')\n",
       "plt.title('Phrase Co-occurrence in Press Briefings')\n",
       "plt.xticks(rotation=45, ha='right')\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Phrase Usage Statistics"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Calculate statistics for each phrase\n",
       "stats_data = []\n",
       "\n",
       "for phrase in SEARCH_PHRASES.keys():\n",
       "    stats = {\n",
       "        'Phrase': phrase,\n",
       "        'Total Mentions': df[phrase].sum(),\n",
       "        'Briefings with Mentions': (df[phrase] > 0).sum(),\n",
       "        'Percentage of Briefings': (df[phrase] > 0).mean() * 100,\n",
       "        'Average per Briefing': df[phrase].mean(),\n",
       "        'Max in Single Briefing': df[phrase].max()\n",
       "    }\n",
       "    stats_data.append(stats)\n",
       "\n",
       "stats_df = pd.DataFrame(stats_data)\n",
       "stats_df = stats_df.sort_values('Total Mentions', ascending=False)\n",
       "\n",
       "# Format the table\n",
       "stats_df['Percentage of Briefings'] = stats_df['Percentage of Briefings'].round(1).astype(str) + '%'\n",
       "stats_df['Average per Briefing'] = stats_df['Average per Briefing'].round(2)\n",
       "\n",
       "display(stats_df)"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   } 