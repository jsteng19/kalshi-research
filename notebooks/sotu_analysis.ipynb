{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trump Speech Analysis for 2025 SOTU Predictions\n",
    "\n",
    "This notebook analyzes the frequency of specific phrases to predict their occurrence in the 2025 SOTU address using:\n",
    "1. Regular speeches (pre-inauguration)\n",
    "2. Post-inauguration speeches (Jan 20, 2025 onwards)\n",
    "3. Previous State of the Union addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('default')\n",
    "sns.set_theme(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_PHRASES = {\n",
    "    'Immigration': r'\\b(illegal\\s+immigra(nt|nts|nt\\'s|nts\\')|immigration)\\b',\n",
    "    'America First': r'\\b(america\\s+first)\\b',\n",
    "    'America': r'\\b(america|america\\'s)\\b', \n",
    "    'Border': r'\\b(border|borders|border\\'s|borders\\')\\b',\n",
    "    'DOGE': r'\\b(doge|doge\\'s|department\\s+of\\s+government\\s+efficiency)\\b',\n",
    "    'AI': r'\\b(ai|ai\\'s|artificial\\s+intelligence|artificial\\s+intelligence\\'s)\\b',\n",
    "    'Ceasefire': r'\\b(ceasefire|ceasefires|ceasefire\\'s|ceasefires\\')\\b',\n",
    "    'Middle Class': r'\\b(middle\\s+class|middle\\s+class\\'s|middle\\s+classes|middle\\s+classes\\')\\b',\n",
    "    'God': r'\\b(god|god\\'s|gods|gods\\')\\b',\n",
    "    'Elon': r'\\b(elon|elon\\'s|elon\\s+musk|elon\\s+musk\\'s)\\b',\n",
    "    'Drill Baby Drill': r'\\bdrill\\s+baby\\s+drill\\b',\n",
    "    'Biden': r'\\b(biden|biden\\'s)\\b',\n",
    "    'Make America Healthy Again': r'\\bmake\\s+america\\s+healthy\\s+again\\b',\n",
    "    'LA': r'\\b(la|la\\'s|los\\s+angeles|los\\s+angeles\\')\\b',\n",
    "    'January 6': r'\\b(january\\s+6(th)?|january\\s+sixth)\\b',\n",
    "    'TikTok': r'\\b(tiktok|tiktok\\'s)\\b',\n",
    "    'Crypto': r'\\b(crypto|crypto\\'s|cryptos|bitcoin|bitcoin\\'s)\\b',\n",
    "    'MAGA': r'\\b(maga|maga\\'s|make\\s+america\\s+great\\s+again)\\b',\n",
    "    'Trans': r'\\b(trans|trans\\')\\b',\n",
    "    'Kamala': r'\\b(kamala|kamala\\'s)\\b',\n",
    "    'Rigged': r'\\b(rig(ged)?)\\b',\n",
    "    'Mandate': r'\\b(mandate|mandates|mandate\\'s|mandates\\')\\b',\n",
    "    'Carnage': r'\\b(carnage|carnage\\'s)\\b'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CONTEXT_WINDOW' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfind_phrase_context\u001b[39m(text, pattern, window\u001b[38;5;241m=\u001b[39m\u001b[43mCONTEXT_WINDOW\u001b[49m):\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find phrase in text with surrounding context\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     matches \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CONTEXT_WINDOW' is not defined"
     ]
    }
   ],
   "source": [
    "def count_phrases(text, phrases=SEARCH_PHRASES):\n",
    "    \"\"\"Count occurrences of phrases in text\"\"\"\n",
    "    counts = {}\n",
    "    for name, pattern in phrases.items():\n",
    "        counts[name] = len(re.findall(pattern, text.lower()))\n",
    "    return counts\n",
    "\n",
    "def get_date_from_filename(filename):\n",
    "    \"\"\"Extract date from filename format YYYY-MM-DD_...\"\"\"\n",
    "    date_str = filename.split('_')[0]\n",
    "    return datetime.strptime(date_str, '%Y-%m-%d')\n",
    "\n",
    "def read_transcript(filepath):\n",
    "    \"\"\"Read and return transcript text\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def find_phrase_context(text, pattern, window=CONTEXT_WINDOW):\n",
    "    \"\"\"Find phrase in text with surrounding context\"\"\"\n",
    "    matches = []\n",
    "    for match in re.finditer(pattern, text.lower()):\n",
    "        start = max(0, match.start() - window)\n",
    "        end = min(len(text), match.end() + window)\n",
    "        context = text[start:end]\n",
    "        # Add ellipsis if we're not at the start/end of the text\n",
    "        if start > 0:\n",
    "            context = '...' + context\n",
    "        if end < len(text):\n",
    "            context = context + '...'\n",
    "        matches.append(context)\n",
    "    return matches\n",
    "\n",
    "def process_directory(directory):\n",
    "    \"\"\"Process transcripts from speech and sotu directories only\"\"\"\n",
    "    results = []\n",
    "    categories_found = set()\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        category = os.path.basename(root)\n",
    "        categories_found.add(category)\n",
    "        \n",
    "        if category in ['speech', 'sotu']:\n",
    "            for file in files:\n",
    "                if file.endswith('.txt'):\n",
    "                    filepath = os.path.join(root, file)\n",
    "                    try:\n",
    "                        date = get_date_from_filename(file)\n",
    "                        text = read_transcript(filepath)\n",
    "                        counts = count_phrases(text)\n",
    "                        \n",
    "                        results.append({\n",
    "                            'date': date,\n",
    "                            'file': file,\n",
    "                            'category': category,\n",
    "                            'text_length': len(text.split()),\n",
    "                            'text': text,  # Store full text for context analysis\n",
    "                            **counts\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {filepath}: {str(e)}\")\n",
    "    \n",
    "    print(\"Categories found in directory:\")\n",
    "    for cat in sorted(categories_found):\n",
    "        print(f\"- {cat}\")\n",
    "    print(\"\\nOnly 'speech' and 'sotu' categories will be analyzed.\\n\")\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process transcripts\n",
    "df = process_directory('../data/processed-transcripts')\n",
    "df = df.sort_values('date')\n",
    "\n",
    "# Split into categories\n",
    "df_pre_inaug = df[(df['category'] == 'speech') & (df['date'] < INAUGURATION_DATE)]\n",
    "df_post_inaug = df[(df['category'] == 'speech') & (df['date'] >= INAUGURATION_DATE)]\n",
    "df_sotu = df[df['category'] == 'sotu']\n",
    "\n",
    "# Print dataset statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Pre-inauguration speeches: {len(df_pre_inaug)}\")\n",
    "print(f\"Post-inauguration speeches: {len(df_post_inaug)}\")\n",
    "print(f\"SOTU addresses: {len(df_sotu)}\")\n",
    "\n",
    "print(\"\\nWord Count Statistics:\")\n",
    "print(f\"Pre-inauguration average length: {df_pre_inaug['text_length'].mean():.0f} words\")\n",
    "print(f\"Post-inauguration average length: {df_post_inaug['text_length'].mean():.0f} words\")\n",
    "print(f\"SOTU average length: {df_sotu['text_length'].mean():.0f} words\")\n",
    "\n",
    "print(\"\\nDate Ranges:\")\n",
    "print(f\"Pre-inauguration: {df_pre_inaug['date'].min().strftime('%Y-%m-%d')} to {df_pre_inaug['date'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"Post-inauguration: {df_post_inaug['date'].min().strftime('%Y-%m-%d')} to {df_post_inaug['date'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"SOTU addresses: {df_sotu['date'].min().strftime('%Y-%m-%d')} to {df_sotu['date'].max().strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Frequency Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_phrase_frequency_over_time(df, phrase, window=30):\n",
    "    \"\"\"Plot the frequency of a phrase over time with a rolling average\"\"\"\n",
    "    # Calculate frequency per 1000 words for each speech\n",
    "    df = df.copy()\n",
    "    df[f'{phrase}_freq'] = (df[phrase] / df['text_length']) * 1000\n",
    "    \n",
    "    # Calculate rolling average\n",
    "    freq_series = df.set_index('date')[f'{phrase}_freq']\n",
    "    rolling_avg = freq_series.rolling(window=f'{window}D', min_periods=1).mean()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.scatter(freq_series.index, freq_series.values, alpha=0.3, label='Individual Speeches')\n",
    "    plt.plot(rolling_avg.index, rolling_avg.values, 'r-', linewidth=2, label=f'{window}-day Rolling Average')\n",
    "    \n",
    "    plt.title(f'Frequency of \"{phrase}\" Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Occurrences per 1000 words')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add vertical line for inauguration\n",
    "    plt.axvline(x=INAUGURATION_DATE, color='k', linestyle='--', alpha=0.5, label='Inauguration')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot time series for top 5 most frequent phrases\n",
    "top_phrases = df.drop(['date', 'file', 'category', 'text_length', 'text'], axis=1).sum().nlargest(5).index\n",
    "\n",
    "for phrase in top_phrases:\n",
    "    plot_phrase_frequency_over_time(df[df['category'] == 'speech'], phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOTU 2025 Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_poisson_predictions(df, avg_length):\n",
    "    \"\"\"Calculate predictions with confidence intervals using Poisson distribution\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for phrase in SEARCH_PHRASES.keys():\n",
    "        # Calculate rate per word\n",
    "        total_occurrences = df[phrase].sum()\n",
    "        total_words = df['text_length'].sum()\n",
    "        rate_per_word = total_occurrences / total_words\n",
    "        \n",
    "        # Expected occurrences in SOTU\n",
    "        expected = rate_per_word * avg_length\n",
    "        \n",
    "        # Calculate confidence intervals\n",
    "        sigma = np.sqrt(expected)  # Standard deviation for Poisson\n",
    "        \n",
    "        predictions.append({\n",
    "            'Phrase': phrase,\n",
    "            'Expected': expected,\n",
    "            'Lower 1σ': max(0, expected - sigma),\n",
    "            'Upper 1σ': expected + sigma,\n",
    "            'Lower 2σ': max(0, expected - 2*sigma),\n",
    "            'Upper 2σ': expected + 2*sigma,\n",
    "            'Historical Rate': rate_per_word * 1000,  # per 1000 words\n",
    "            'Total Historical': total_occurrences\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(predictions)\n",
    "\n",
    "# Use SOTU average length for predictions\n",
    "avg_sotu_length = df_sotu['text_length'].mean()\n",
    "\n",
    "# Calculate predictions from all sources\n",
    "predictions_pre_inaug = calculate_poisson_predictions(df_pre_inaug, avg_sotu_length)\n",
    "predictions_post_inaug = calculate_poisson_predictions(df_post_inaug, avg_sotu_length)\n",
    "predictions_sotu = calculate_poisson_predictions(df_sotu, avg_sotu_length)\n",
    "\n",
    "# Format the tables\n",
    "def format_prediction_table(df, source):\n",
    "    formatted = df.copy()\n",
    "    formatted = formatted.round(2)\n",
    "    formatted['68% CI'] = formatted.apply(lambda x: f\"({x['Lower 1σ']:.1f} - {x['Upper 1σ']:.1f})\", axis=1)\n",
    "    formatted['95% CI'] = formatted.apply(lambda x: f\"({x['Lower 2σ']:.1f} - {x['Upper 2σ']:.1f})\", axis=1)\n",
    "    \n",
    "    return formatted[[\n",
    "        'Phrase', 'Expected', '68% CI', '95% CI', \n",
    "        'Historical Rate', 'Total Historical'\n",
    "    ]].sort_values('Expected', ascending=False)\n",
    "\n",
    "print(\"Predictions based on pre-inauguration speeches:\")\n",
    "display(format_prediction_table(predictions_pre_inaug, 'pre-inauguration'))\n",
    "\n",
    "print(\"\\nPredictions based on post-inauguration speeches:\")\n",
    "display(format_prediction_table(predictions_post_inaug, 'post-inauguration'))\n",
    "\n",
    "print(\"\\nPredictions based on previous SOTU addresses:\")\n",
    "display(format_prediction_table(predictions_sotu, 'sotu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recent Usage Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recent_contexts(df, phrase, n=5):\n",
    "    \"\"\"Get the n most recent contexts for a phrase\"\"\"\n",
    "    # Create a list to store matches with their dates\n",
    "    all_matches = []\n",
    "    \n",
    "    # Look through speeches from newest to oldest\n",
    "    for _, row in df.sort_values('date', ascending=False).iterrows():\n",
    "        matches = find_phrase_context(row['text'], SEARCH_PHRASES[phrase])\n",
    "        for match in matches:\n",
    "            all_matches.append({\n",
    "                'date': row['date'],\n",
    "                'category': row['category'],\n",
    "                'context': match\n",
    "            })\n",
    "        if len(all_matches) >= n:\n",
    "            break\n",
    "    \n",
    "    return pd.DataFrame(all_matches[:n])\n",
    "\n",
    "# Get recent contexts for each phrase\n",
    "for phrase in SEARCH_PHRASES.keys():\n",
    "    contexts = get_recent_contexts(df, phrase)\n",
    "    if not contexts.empty:\n",
    "        print(f\"\\n=== Recent usage of '{phrase}' ===\\n\")\n",
    "        for _, row in contexts.iterrows():\n",
    "            print(f\"Date: {row['date'].strftime('%Y-%m-%d')} ({row['category']})\")\n",
    "            print(f\"Context: {row['context']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
