{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trump Speech Analysis for 2025 SOTU Predictions\n",
    "\n",
    "This notebook analyzes the frequency of specific phrases to predict their occurrence in the 2025 SOTU address using:\n",
    "1. Regular speeches (pre-inauguration)\n",
    "2. Post-inauguration speeches (Jan 20, 2025 onwards)\n",
    "3. Previous State of the Union addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('default')\n",
    "sns.set_theme(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_WINDOW = 500  # Number of characters before/after for context\n",
    "INAUGURATION_DATE = datetime(2025, 1, 20)  # Trump's hypothetical 2025 inauguration date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrases to Track\n",
    "\n",
    "- Illegal Immigrant / Immigration\n",
    "- America First\n",
    "- DOGE / Department of Government Efficiency \n",
    "- America (15+ times)\n",
    "- Israel\n",
    "- Border (5+ times)\n",
    "- AI / Artificial Intelligence\n",
    "- Canada\n",
    "- Mexico\n",
    "- Middle Class\n",
    "- Ceasefire\n",
    "- Gaza\n",
    "- God (4+ times)\n",
    "- Biden\n",
    "- Panama\n",
    "- Elon / Elon Musk\n",
    "- Drill Baby Drill\n",
    "- LA / Los Angeles\n",
    "- Make America Healthy Again\n",
    "- January 6\n",
    "- TikTok\n",
    "- Crypto / Bitcoin\n",
    "- Hell\n",
    "- Kamala\n",
    "- Trans\n",
    "- Greenland\n",
    "- MAGA / Make America Great Again (4+ times)\n",
    "- Rig / Rigged\n",
    "- Mandate (3+ times)\n",
    "- Carnage\n",
    "- Doge / Dogecoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude don't touch this cell!\n",
    "SEARCH_PHRASES = {\n",
    "    'Immigration': r'\\b(illegal\\s+immigra(nt|nts|nt\\'s|nts\\')|immigration)\\b',\n",
    "    'America First': r'\\b(america\\s+first)\\b',\n",
    "    'America': r'\\b(america|america\\'s)\\b', \n",
    "    'Border': r'\\b(border|borders|border\\'s|borders\\')\\b',\n",
    "    'DOGE': r'\\b(doge|doge\\'s|department\\s+of\\s+government\\s+efficiency)\\b',\n",
    "    'AI': r'\\b(ai|ai\\'s|artificial\\s+intelligence|artificial\\s+intelligence\\'s)\\b',\n",
    "    'Ceasefire': r'\\b(ceasefire|ceasefires|ceasefire\\'s|ceasefires\\')\\b',\n",
    "    'Middle Class': r'\\b(middle\\s+class|middle\\s+class\\'s|middle\\s+classes|middle\\s+classes\\')\\b',\n",
    "    'God': r'\\b(god|god\\'s|gods|gods\\')\\b',\n",
    "    'Elon': r'\\b(elon|elon\\'s|elon\\s+musk|elon\\s+musk\\'s)\\b',\n",
    "    'Drill Baby Drill': r'\\bdrill\\s+baby\\s+drill\\b',\n",
    "    'Biden': r'\\b(biden|biden\\'s)\\b',\n",
    "    'Make America Healthy Again': r'\\bmake\\s+america\\s+healthy\\s+again\\b',\n",
    "    'LA': r'\\b(LA|LA\\'s|los\\s+angeles|los\\s+angeles\\')\\b',\n",
    "    'January 6': r'\\b(january\\s+6(th)?|january\\s+sixth)\\b',\n",
    "    'TikTok': r'\\b(tiktok|tiktok\\'s)\\b',\n",
    "    'Crypto': r'\\b(crypto|crypto\\'s|cryptos|bitcoin|bitcoin\\'s)\\b',\n",
    "    'MAGA': r'\\b(maga|maga\\'s|make\\s+america\\s+great\\s+again)\\b',\n",
    "    'Trans': r'\\b(trans)(?!-)\\b',\n",
    "    'Kamala': r'\\b(kamala|kamala\\'s)\\b',\n",
    "    'Rigged': r'\\b(rig(ged)?)\\b',\n",
    "    'Mandate': r'\\b(mandate|mandates|mandate\\'s|mandates\\')\\b',\n",
    "    'Carnage': r'\\b(carnage|carnage\\'s)\\b',\n",
    "    'Israel': r'\\b(israel|israel\\'s)\\b',\n",
    "    'Hell': r'\\b(hell|hell\\'s)\\b',\n",
    "    'Greenland': r'\\b(greenland|greenland\\'s)\\b',\n",
    "    'Mexico': r'\\b(mexico|mexico\\'s)\\b',\n",
    "    'Canada': r'\\b(canada|canada\\'s)\\b',\n",
    "    'Panama': r'\\b(panama|panama\\'s)\\b',\n",
    "    'Gaza': r'\\b(gaza|gaza\\'s)\\b',\n",
    "    'FEMA': r'\\b(fema|fema\\'s)\\b'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_phrases(text, phrases=SEARCH_PHRASES):\n",
    "    \"\"\"Count occurrences of phrases in text\"\"\"\n",
    "    counts = {}\n",
    "    for name, pattern in phrases.items():\n",
    "        counts[name] = len(re.findall(pattern, text.lower()))\n",
    "    return counts\n",
    "\n",
    "def get_date_from_filename(filename):\n",
    "    \"\"\"Extract date from filename format YYYY-MM-DD_...\"\"\"\n",
    "    date_str = filename.split('_')[0]\n",
    "    return datetime.strptime(date_str, '%Y-%m-%d')\n",
    "\n",
    "def read_transcript(filepath):\n",
    "    \"\"\"Read and return transcript text\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def find_phrase_context(text, pattern, window=CONTEXT_WINDOW):\n",
    "    \"\"\"Find phrase in text with surrounding context\"\"\"\n",
    "    matches = []\n",
    "    for match in re.finditer(pattern, text.lower()):\n",
    "        start = max(0, match.start() - window)\n",
    "        end = min(len(text), match.end() + window)\n",
    "        context = text[start:end]\n",
    "        # Add ellipsis if we're not at the start/end of the text\n",
    "        if start > 0:\n",
    "            context = '...' + context\n",
    "        if end < len(text):\n",
    "            context = context + '...'\n",
    "        matches.append(context)\n",
    "    return matches\n",
    "\n",
    "def process_directory(directory):\n",
    "    \"\"\"Process transcripts from speech and sotu directories only\"\"\"\n",
    "    results = []\n",
    "    categories_found = set()\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        category = os.path.basename(root)\n",
    "        categories_found.add(category)\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                filepath = os.path.join(root, file)\n",
    "                try:\n",
    "                    date = get_date_from_filename(file)\n",
    "                    text = read_transcript(filepath)\n",
    "                    counts = count_phrases(text)\n",
    "                    \n",
    "                    results.append({\n",
    "                        'date': date,\n",
    "                        'file': file,\n",
    "                        'category': category,\n",
    "                        'text_length': len(text.split()),\n",
    "                        'text': text,  # Store full text for context analysis\n",
    "                        **counts\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filepath}: {str(e)}\")\n",
    "    \n",
    "    print(\"Categories found in directory:\")\n",
    "    for cat in sorted(categories_found):\n",
    "        print(f\"- {cat}\")\n",
    "    print(\"\\nOnly 'speech' and 'sotu' categories will be analyzed.\\n\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process transcripts\n",
    "df = process_directory('../data/processed-transcripts')\n",
    "df = df.sort_values('date')\n",
    "\n",
    "# Split into categories for speeches\n",
    "df_speech_pre = df[(df['category'] == 'speech') & (df['date'] < INAUGURATION_DATE)]\n",
    "df_speech_post = df[(df['category'] == 'speech') & (df['date'] >= INAUGURATION_DATE)]\n",
    "df_speech = pd.concat([df_speech_pre, df_speech_post])\n",
    "\n",
    "# Split into categories for non-speeches and SOTU\n",
    "df_nonspeech_pre = df[(df['category'] != 'speech') & (df['category'] != 'sotu') & (df['date'] < INAUGURATION_DATE)]\n",
    "df_nonspeech_post = df[(df['category'] != 'speech') & (df['category'] != 'sotu') & (df['date'] >= INAUGURATION_DATE)]\n",
    "df_nonspeech = pd.concat([df_nonspeech_pre, df_nonspeech_post])\n",
    "df_sotu = df[df['category'] == 'sotu']\n",
    "\n",
    "# Print dataset statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(\"Speeches:\")\n",
    "print(f\"Pre-inauguration: {len(df_speech_pre)}\")\n",
    "print(f\"Post-inauguration: {len(df_speech_post)}\")\n",
    "print(\"\\nNon-speeches:\")\n",
    "print(f\"Pre-inauguration: {len(df_nonspeech_pre)}\")\n",
    "print(f\"Post-inauguration: {len(df_nonspeech_post)}\")\n",
    "print(\"\\nState of the Union:\")\n",
    "print(f\"Total: {len(df_sotu)}\")\n",
    "\n",
    "print(\"\\nWord Count Statistics:\")\n",
    "print(\"Speeches:\")\n",
    "print(f\"Pre-inauguration average length: {df_speech_pre['text_length'].mean():.0f} words\")\n",
    "print(f\"Post-inauguration average length: {df_speech_post['text_length'].mean():.0f} words\")\n",
    "print(\"\\nNon-speeches:\")\n",
    "print(f\"Pre-inauguration average length: {df_nonspeech_pre['text_length'].mean():.0f} words\") \n",
    "print(f\"Post-inauguration average length: {df_nonspeech_post['text_length'].mean():.0f} words\")\n",
    "print(\"\\nState of the Union:\")\n",
    "print(f\"Average length: {df_sotu['text_length'].mean():.0f} words\")\n",
    "\n",
    "print(\"\\nDate Ranges:\")\n",
    "print(\"Speeches:\")\n",
    "print(f\"Pre-inauguration: {df_speech_pre['date'].min().strftime('%Y-%m-%d')} to {df_speech_pre['date'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"Post-inauguration: {df_speech_post['date'].min().strftime('%Y-%m-%d')} to {df_speech_post['date'].max().strftime('%Y-%m-%d')}\")\n",
    "print(\"\\nNon-speeches:\")\n",
    "print(f\"Pre-inauguration: {df_nonspeech_pre['date'].min().strftime('%Y-%m-%d')} to {df_nonspeech_pre['date'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"Post-inauguration: {df_nonspeech_post['date'].min().strftime('%Y-%m-%d')} to {df_nonspeech_post['date'].max().strftime('%Y-%m-%d')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Frequency Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_phrase_frequency_over_time(df_speech, df_non_speech, phrase, window=30):\n",
    "    \"\"\"Plot the frequency of a phrase over time with separate lines for speech and non-speech\"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Process speech data\n",
    "    df_speech = df_speech.copy()\n",
    "    df_speech[f'{phrase}_freq'] = (df_speech[phrase] / df_speech['text_length']) * 1000\n",
    "    speech_series = df_speech.set_index('date')[f'{phrase}_freq']\n",
    "    speech_rolling = speech_series.rolling(window=f'{window}D', min_periods=1).mean()\n",
    "    \n",
    "    # Process non-speech data\n",
    "    df_non_speech = df_non_speech.copy()\n",
    "    df_non_speech[f'{phrase}_freq'] = (df_non_speech[phrase] / df_non_speech['text_length']) * 1000\n",
    "    non_speech_series = df_non_speech.set_index('date')[f'{phrase}_freq']\n",
    "    non_speech_rolling = non_speech_series.rolling(window=f'{window}D', min_periods=1).mean()\n",
    "    \n",
    "    # Plot speech data\n",
    "    plt.scatter(speech_series.index, speech_series.values, alpha=0.3, color='red', label='Speech Transcripts')\n",
    "    plt.plot(speech_rolling.index, speech_rolling.values, 'r-', linewidth=2, label=f'Speech {window}-day Average')\n",
    "    \n",
    "    # Plot non-speech data\n",
    "    plt.scatter(non_speech_series.index, non_speech_series.values, alpha=0.3, color='blue', label='Non-Speech Transcripts')\n",
    "    plt.plot(non_speech_rolling.index, non_speech_rolling.values, 'b-', linewidth=2, label=f'Non-Speech {window}-day Average')\n",
    "    \n",
    "    plt.title(f'Frequency of \"{phrase}\" Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Occurrences per 1000 words')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add vertical line for inauguration\n",
    "    plt.axvline(x=INAUGURATION_DATE, color='k', linestyle='--', alpha=0.5, label='Inauguration')\n",
    "    \n",
    "    # Auto-adjust y-axis limit up to max of 8\n",
    "    ymax = min(8, max(\n",
    "        df_speech[f'{phrase}_freq'].max(),\n",
    "        df_non_speech[f'{phrase}_freq'].max()\n",
    "    ) * 1.1)  # Add 10% padding\n",
    "    plt.ylim(0, ymax)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot time series for all phrases, sorted by frequency\n",
    "phrase_freqs = {}\n",
    "for phrase in SEARCH_PHRASES.keys():\n",
    "    # Calculate average frequency across all data\n",
    "    speech_freq = (df_speech[phrase].sum() / df_speech['text_length'].sum()) * 1000\n",
    "    nonspeech_freq = (df_nonspeech[phrase].sum() / df_nonspeech['text_length'].sum()) * 1000\n",
    "    phrase_freqs[phrase] = (speech_freq + nonspeech_freq) / 2\n",
    "\n",
    "# Sort phrases by frequency and plot\n",
    "for phrase in sorted(phrase_freqs, key=phrase_freqs.get, reverse=True):\n",
    "    plot_phrase_frequency_over_time(df_speech, df_nonspeech, phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous State of the Unions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table showing phrase counts for each SOTU\n",
    "sotu_counts = []\n",
    "\n",
    "for phrase in SEARCH_PHRASES.keys():\n",
    "    # Get counts for each SOTU\n",
    "    counts = df_sotu[['date', phrase]].values.tolist()\n",
    "    \n",
    "    # Format into a row with the phrase and counts\n",
    "    row = {'Phrase': phrase}\n",
    "    for date, count in counts:\n",
    "        year = date.year\n",
    "        row[f'SOTU {year}'] = count\n",
    "        \n",
    "    sotu_counts.append(row)\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "df_sotu_counts = pd.DataFrame(sotu_counts)\n",
    "# Sort by average counts across all SOTUs\n",
    "df_sotu_counts['Average'] = df_sotu_counts[[col for col in df_sotu_counts.columns if 'SOTU' in col]].mean(axis=1)\n",
    "df_sotu_counts = df_sotu_counts.sort_values('Average', ascending=False)\n",
    "\n",
    "# Display the table\n",
    "display(df_sotu_counts.style.format({col: '{:.0f}' for col in df_sotu_counts.columns if 'SOTU' in col}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOTU 2025 Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_poisson_predictions(df, avg_length):\n",
    "    \"\"\"Calculate predictions with confidence intervals and likelihoods using Poisson distribution\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for phrase in SEARCH_PHRASES.keys():\n",
    "        # Calculate rate per word\n",
    "        total_occurrences = df[phrase].sum()\n",
    "        total_words = df['text_length'].sum()\n",
    "        rate_per_word = total_occurrences / total_words\n",
    "        \n",
    "        # Expected occurrences in SOTU\n",
    "        expected = rate_per_word * avg_length\n",
    "        \n",
    "        # Calculate confidence intervals\n",
    "        sigma = np.sqrt(expected)  # Standard deviation for Poisson\n",
    "        \n",
    "        # Calculate likelihoods using Poisson PMF\n",
    "        def poisson_ge_k(lambda_, k):\n",
    "            return 1 - stats.poisson.cdf(k-1, lambda_)\n",
    "        \n",
    "        predictions.append({\n",
    "            'Phrase': phrase,\n",
    "            'Expected': expected,\n",
    "            'Lower 1σ': max(0, expected - sigma),\n",
    "            'Upper 1σ': expected + sigma,\n",
    "            'Lower 2σ': max(0, expected - 2*sigma),\n",
    "            'Upper 2σ': expected + 2*sigma,\n",
    "            'Historical Rate': rate_per_word * 1000,  # per 1000 words\n",
    "            'Total Historical': total_occurrences,\n",
    "            'P(≥1)': poisson_ge_k(expected, 1),\n",
    "            'P(≥3)': poisson_ge_k(expected, 3),\n",
    "            'P(≥4)': poisson_ge_k(expected, 4),\n",
    "            'P(≥5)': poisson_ge_k(expected, 5),\n",
    "            'P(≥15)': poisson_ge_k(expected, 15)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(predictions)\n",
    "\n",
    "# Use SOTU average length for predictions\n",
    "avg_sotu_length = df_sotu['text_length'].mean()\n",
    "\n",
    "# Calculate predictions from all sources\n",
    "predictions_pre_inaug = calculate_poisson_predictions(df_speech_pre, avg_sotu_length)\n",
    "predictions_post_inaug = calculate_poisson_predictions(df_speech_post, avg_sotu_length)\n",
    "predictions_sotu = calculate_poisson_predictions(df_sotu, avg_sotu_length)\n",
    "\n",
    "# Format the tables\n",
    "def format_prediction_table(df, source):\n",
    "    formatted = df.copy()\n",
    "    formatted = formatted.round(2)\n",
    "    formatted['68% CI'] = formatted.apply(lambda x: f\"({x['Lower 1σ']:.1f} - {x['Upper 1σ']:.1f})\", axis=1)\n",
    "    formatted['95% CI'] = formatted.apply(lambda x: f\"({x['Lower 2σ']:.1f} - {x['Upper 2σ']:.1f})\", axis=1)\n",
    "    formatted['P(≥1)'] = formatted['P(≥1)'].apply(lambda x: f\"{x:.1%}\")\n",
    "    formatted['P(≥3)'] = formatted['P(≥3)'].apply(lambda x: f\"{x:.1%}\")\n",
    "    formatted['P(≥4)'] = formatted['P(≥4)'].apply(lambda x: f\"{x:.1%}\")\n",
    "    formatted['P(≥5)'] = formatted['P(≥5)'].apply(lambda x: f\"{x:.1%}\")\n",
    "    formatted['P(≥15)'] = formatted['P(≥15)'].apply(lambda x: f\"{x:.1%}\")\n",
    "    \n",
    "    return formatted[[\n",
    "        'Phrase', 'Expected', '68% CI', '95% CI', \n",
    "        'P(≥1)', 'P(≥3)', 'P(≥4)', 'P(≥5)', 'P(≥15)',\n",
    "        'Historical Rate', 'Total Historical'\n",
    "    ]].sort_values('Expected', ascending=False)\n",
    "\n",
    "print(\"Predictions based on pre-inauguration speeches:\")\n",
    "display(format_prediction_table(predictions_pre_inaug, 'pre-inauguration'))\n",
    "\n",
    "print(\"\\nPredictions based on post-inauguration speeches:\")\n",
    "display(format_prediction_table(predictions_post_inaug, 'post-inauguration'))\n",
    "\n",
    "print(\"\\nPredictions based on previous SOTU addresses:\")\n",
    "display(format_prediction_table(predictions_sotu, 'sotu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recent Usage Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recent_contexts(df, phrase, n=5):\n",
    "    \"\"\"Get the n most recent contexts for a phrase\"\"\"\n",
    "    # Create a list to store matches with their dates\n",
    "    all_matches = []\n",
    "    \n",
    "    # Look through speeches from newest to oldest\n",
    "    for _, row in df.sort_values('date', ascending=False).iterrows():\n",
    "        matches = find_phrase_context(row['text'], SEARCH_PHRASES[phrase])\n",
    "        for match in matches:\n",
    "            all_matches.append({\n",
    "                'date': row['date'],\n",
    "                'category': row['category'],\n",
    "                'context': match\n",
    "            })\n",
    "        if len(all_matches) >= n:\n",
    "            break\n",
    "    \n",
    "    return pd.DataFrame(all_matches[:n])\n",
    "\n",
    "# Get recent contexts for each phrase\n",
    "for phrase in SEARCH_PHRASES.keys():\n",
    "    contexts = get_recent_contexts(df, phrase)\n",
    "    if not contexts.empty:\n",
    "        print(f\"\\n=== Recent usage of '{phrase}' ===\\n\")\n",
    "        for _, row in contexts.iterrows():\n",
    "            print(f\"Date: {row['date'].strftime('%Y-%m-%d')} ({row['category']})\")\n",
    "            print(f\"Context: {row['context']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recent_contexts(df, 'Trans', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Speech Frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_speech_frequencies(df, sotu_mean_length):\n",
    "    \"\"\"Analyze frequency of phrases in individual speeches\"\"\"\n",
    "    # Filter out length outliers (more than 2x SOTU length)\n",
    "    df_filtered = df[df['text_length'] <= 2 * sotu_mean_length].copy()\n",
    "    \n",
    "    results = []\n",
    "    for phrase in SEARCH_PHRASES.keys():\n",
    "        result = {\n",
    "            'Phrase': phrase,\n",
    "            'Total Speeches': len(df_filtered),\n",
    "            'P(≥1)': (df_filtered[phrase] >= 1).mean(),\n",
    "            'P(≥3)': (df_filtered[phrase] >= 3).mean(),\n",
    "            'P(≥4)': (df_filtered[phrase] >= 4).mean(),\n",
    "            'P(≥5)': (df_filtered[phrase] >= 5).mean(),\n",
    "            'P(≥15)': (df_filtered[phrase] >= 15).mean(),\n",
    "            'Max Occurrences': df_filtered[phrase].max(),\n",
    "            'Mean Occurrences': df_filtered[phrase].mean()\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotu_mean_length = df_sotu['text_length'].mean()\n",
    "\n",
    "# Plot distribution of speech lengths with SOTU average marked\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(df['text_length'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.axvline(x=sotu_mean_length, color='red', linestyle='--', label=f'Avg SOTU Length ({int(sotu_mean_length):,} words)')\n",
    "plt.xlabel('Speech Length (words)')\n",
    "plt.ylabel('Number of Speeches')\n",
    "plt.title('Distribution of Trump Speech Lengths')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display per-speech frequencies\n",
    "speech_freq_df = analyze_speech_frequencies(df_speech, sotu_mean_length)\n",
    "\n",
    "# Sort by probability of at least one occurrence\n",
    "speech_freq_df = speech_freq_df.sort_values('P(≥1)', ascending=False)\n",
    "\n",
    "# Format percentages\n",
    "for col in ['P(≥1)', 'P(≥3)', 'P(≥4)', 'P(≥5)', 'P(≥15)']:\n",
    "    speech_freq_df[col] = speech_freq_df[col].map('{:.1%}'.format)\n",
    "\n",
    "# Display results\n",
    "speech_freq_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Phrase Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find speeches shorter than 6000 words with 4+ mentions of MAGA\n",
    "short_maga_speeches = df[\n",
    "    (df['text_length'] < 10200) & \n",
    "    (df['MAGA'] >= 4)\n",
    "][['file', 'text_length', 'MAGA']]\n",
    "\n",
    "print(f\"\\nSpeeches under 10200 words with 4+ mentions of MAGA:\")\n",
    "print(short_maga_speeches.sort_values('MAGA', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['January 6'] > 0][['file', 'text_length', 'January 6', 'category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['LA'] > 0][['file', 'text_length', 'LA', 'category']].sort_values('file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date strings to datetime \n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Create weekly bins\n",
    "df['week'] = df['date'].dt.to_period('W')\n",
    "\n",
    "# Group by week and calculate metrics\n",
    "weekly_stats = df.groupby('week').agg({\n",
    "    'file': 'count',  # Total speeches per week\n",
    "    'LA': lambda x: (x > 0).sum()  # Speeches containing LA per week\n",
    "}).reset_index()\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot bars for speech counts\n",
    "ax.bar(range(len(weekly_stats)), weekly_stats['file'], alpha=0.3, color='gray', label='Total Speeches')\n",
    "ax.bar(range(len(weekly_stats)), weekly_stats['LA'], alpha=0.6, color='blue', label='Speeches with LA')\n",
    "\n",
    "# Customize axes\n",
    "ax.set_xlabel('Week')\n",
    "ax.set_ylabel('Number of Speeches')\n",
    "\n",
    "# Set x-axis ticks to show dates every 2 weeks\n",
    "tick_indices = range(0, len(weekly_stats), 2)\n",
    "plt.xticks(tick_indices, [str(weekly_stats['week'].iloc[i]) for i in tick_indices], rotation=45)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "plt.title('Weekly Frequency of LA Mentions in Speeches')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
